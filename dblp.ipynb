{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UK Doctoral Thesis Metadata from EThOS\n",
    "\n",
    "The data in this collection comprises the bibliographic metadata for all UK doctoral theses listed in EThOS, the UK's national thesis service.\n",
    "\n",
    "\n",
    "https://dblp.uni-trier.de/xml/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/gustavo/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import re\n",
    "import os\n",
    "from pathlib import Path\n",
    "import requests\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy import mean, ones\n",
    "from scipy.sparse import csr_matrix\n",
    "import io\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we retrieve the dataset directly from the British Library repository. We can also download the file in our system. The parameter name sets the fields that we will find in the CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('dblp/dblp.csv.gz', compression='gzip', header=0, sep='\\t', quotechar='\"', error_bad_lines=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>TYPE</th>\n",
       "      <th>TITLE</th>\n",
       "      <th>YEAR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tr/meltdown/s18</td>\n",
       "      <td>article</td>\n",
       "      <td>Spectre Attacks: Exploiting Speculative Execut...</td>\n",
       "      <td>2018.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>tr/meltdown/m18</td>\n",
       "      <td>article</td>\n",
       "      <td>Meltdown</td>\n",
       "      <td>2018.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>tr/acm/CS2013</td>\n",
       "      <td>book</td>\n",
       "      <td>Computer Science Curricula 2013</td>\n",
       "      <td>2013.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>tr/gte/TR-0263-08-94-165</td>\n",
       "      <td>article</td>\n",
       "      <td>An Evaluation of Object-Oriented DBMS Developm...</td>\n",
       "      <td>1994.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>tr/gte/TR-0222-10-92-165</td>\n",
       "      <td>article</td>\n",
       "      <td>DARWIN: On the Incremental Migration of Legacy...</td>\n",
       "      <td>1993.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8219991</th>\n",
       "      <td>phd/Smolka89</td>\n",
       "      <td>phdthesis</td>\n",
       "      <td>Logic Programming over Polymorphically Order-S...</td>\n",
       "      <td>1989.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8219992</th>\n",
       "      <td>phd/Dobry87</td>\n",
       "      <td>phdthesis</td>\n",
       "      <td>A High Performance Architecture For Prolog</td>\n",
       "      <td>1987.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8219993</th>\n",
       "      <td>phd/Ghemawat95</td>\n",
       "      <td>phdthesis</td>\n",
       "      <td>The Modified Object Buffer: A Storage Manageme...</td>\n",
       "      <td>1995.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8219994</th>\n",
       "      <td>phd/Rothkugel2002</td>\n",
       "      <td>phdthesis</td>\n",
       "      <td>Towards Middleware Support for Mobile and Cell...</td>\n",
       "      <td>2002.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8219995</th>\n",
       "      <td>phd/sk/Frisch2009</td>\n",
       "      <td>phdthesis</td>\n",
       "      <td>Using open source software to develop E-busine...</td>\n",
       "      <td>2009.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8219996 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                       Unnamed: 0       TYPE  \\\n",
       "0                 tr/meltdown/s18    article   \n",
       "1                 tr/meltdown/m18    article   \n",
       "2                   tr/acm/CS2013       book   \n",
       "3        tr/gte/TR-0263-08-94-165    article   \n",
       "4        tr/gte/TR-0222-10-92-165    article   \n",
       "...                           ...        ...   \n",
       "8219991              phd/Smolka89  phdthesis   \n",
       "8219992               phd/Dobry87  phdthesis   \n",
       "8219993            phd/Ghemawat95  phdthesis   \n",
       "8219994         phd/Rothkugel2002  phdthesis   \n",
       "8219995         phd/sk/Frisch2009  phdthesis   \n",
       "\n",
       "                                                     TITLE    YEAR  \n",
       "0        Spectre Attacks: Exploiting Speculative Execut...  2018.0  \n",
       "1                                                 Meltdown  2018.0  \n",
       "2                          Computer Science Curricula 2013  2013.0  \n",
       "3        An Evaluation of Object-Oriented DBMS Developm...  1994.0  \n",
       "4        DARWIN: On the Incremental Migration of Legacy...  1993.0  \n",
       "...                                                    ...     ...  \n",
       "8219991  Logic Programming over Polymorphically Order-S...  1989.0  \n",
       "8219992         A High Performance Architecture For Prolog  1987.0  \n",
       "8219993  The Modified Object Buffer: A Storage Manageme...  1995.0  \n",
       "8219994  Towards Middleware Support for Mobile and Cell...  2002.0  \n",
       "8219995  Using open source software to develop E-busine...  2009.0  \n",
       "\n",
       "[8219996 rows x 4 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see the last year in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2021.0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.YEAR.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see the different values of Type in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['article', 'book', 'proceedings', 'inproceedings', 'www',\n",
       "       'mastersthesis', 'incollection', 'phdthesis'], dtype=object)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.TYPE.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's filter the list of results typed as article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles = df[df['TYPE'] == \"article\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can filter the list of results by year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles_2002 = articles[articles.YEAR == 2002]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles_2002.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles_2003 = articles[articles.YEAR == 2003]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles_2003"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles_2010_2021 = articles[articles.YEAR >= 2000 && articles.YEAR <= 2010]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles_2010_2021.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MPHash(object):\n",
    "    # create from iterable \n",
    "    def __init__(self, terms):\n",
    "        self.term = list(terms)\n",
    "        self.code = {t:n for n, t in enumerate(self.term)}\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.term)\n",
    "    \n",
    "    def get_code(self, term):\n",
    "        return self.code.get(term)\n",
    "    \n",
    "    def get_term(self, code):\n",
    "        return self.term[code]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A sample is a collection of texts and publication dates \n",
    "# For each text, the sample stores its year and word counts. \n",
    "class Sample(object):\n",
    "    pattern = pattern = r\"(?:\\w+[-])*\\w*[^\\W\\d_]\\w*(?:[-'’`]\\w+)*\"\n",
    "    # Create Sample from data stored in a DataFrame with at least columns \n",
    "    # TEXT, YEAR\n",
    "    # n = maximal ngram size \n",
    "    def __init__(self, data, ngram_length):\n",
    "        self.size = len(data)\n",
    "        self.year = data.YEAR.tolist()\n",
    "        \n",
    "        texts = tuple(data.TITLE)\n",
    "        vectorizer = CountVectorizer(token_pattern = Sample.pattern, \n",
    "                                     stop_words=stopwords.words('english'),\n",
    "                                     max_df=0.1,\n",
    "                                     ngram_range=(1, ngram_length))\n",
    "        matrix = vectorizer.fit_transform(texts).transpose() \n",
    "        # remove all hapax legomena to save space\n",
    "        terms = vectorizer.get_feature_names()\n",
    "        frequencies = matrix.sum(axis=1).A1\n",
    "        selected = [m for m, f in enumerate(frequencies) if f > 1]\n",
    "        hapax_rate = 1 - len(selected) / len(frequencies)\n",
    "        print('Removing hapax legomena ({:.1f}%)'.format(100 * hapax_rate))\n",
    "        self.matrix = matrix[selected, :]      \n",
    "        self.term_codes = MPHash([terms[m] for m in selected])\n",
    "        \n",
    "        # store array with global term frequencies\n",
    "        self.term_frequencies = self.matrix.sum(axis=1).A1\n",
    "        # store doc frequencies\n",
    "        self.doc_frequencies = self.matrix.getnnz(axis=1)\n",
    "        # store most common capitalization of terms\n",
    "        print('Obtaining most common capitalizations')\n",
    "        vectorizer.lowercase = False\n",
    "        matrix = vectorizer.fit_transform(texts).transpose()\n",
    "        terms = vectorizer.get_feature_names()\n",
    "        frequencies = matrix.sum(axis=1).A1    \n",
    "        forms = dict()\n",
    "        for t, f in zip(terms, frequencies):\n",
    "            low = t.lower()\n",
    "            if forms.get(low, (None, 0))[1] < f:\n",
    "                forms[low] = (t, f)\n",
    "        self.capitals = {k:v[0] for k, v in forms.items()}\n",
    "        \n",
    "        print('Computed stats for', len(self.term_codes), 'terms')\n",
    "        \n",
    "    # return the number of texts stored in this Sample\n",
    "    def __len__(self):\n",
    "        return self.size\n",
    "    \n",
    "    # return term frequency of the specified term\n",
    "    def get_tf(self, term):\n",
    "        code = self.term_codes.get_code(term.lower())\n",
    "        \n",
    "        return self.term_frequencies[code]\n",
    "    \n",
    "    # return document frequency of the specified term\n",
    "    def get_df(self, term):\n",
    "         code = self.term_codes.get_code(term.lower())\n",
    "         \n",
    "         return self.doc_frequencies[code]\n",
    "     \n",
    "    # return the most frequent capitalization form\n",
    "    # (also for stopwords not in dictionary)\n",
    "    def most_frequent_capitalization(self, term):\n",
    "        return self.capitals.get(term.lower(), term)\n",
    "    \n",
    "    # return the average submission year of texts containing every term\n",
    "    def average_year(self, period, tf_threshold=20, df_threshold=3):\n",
    "        docs = [n for n, y in enumerate(self.year)\\\n",
    "                if period[0] <= y <= period[1]]\n",
    "        tf_matrix = self.matrix[:, docs]\n",
    "        tf_sum = tf_matrix.sum(axis=1).A1\n",
    "        df_sum = tf_matrix.getnnz(axis=1)\n",
    "        terms = [m for m, tf in enumerate(tf_sum)\\\n",
    "                 if tf >= tf_threshold and df_sum[m] >= df_threshold]\n",
    "        tf_matrix = tf_matrix[terms, :]     \n",
    "        rows, cols = tf_matrix.nonzero()\n",
    "        df_matrix = csr_matrix((ones(len(rows)), (rows, cols)))\n",
    "        year = [self.year[n] for n in docs]\n",
    "        \n",
    "        res = df_matrix @ year / df_matrix.getnnz(axis=1) # @ operator = matrix multiplication\n",
    "        \n",
    "        return {self.term_codes.get_term(terms[m]):res[m] for m in range(len(res))}\n",
    "\n",
    "        \n",
    "    # return the number of occurrences (doc frequency) for every term \n",
    "    def get_df_per_year(self, term):\n",
    "        m = self.term_codes.get_code(term)\n",
    "        row = self.matrix.getrow(m)\n",
    "        _, docs = row.nonzero()\n",
    "        c = Counter(map(self.year.__getitem__, docs))\n",
    "\n",
    "        return c\n",
    "          \n",
    "    # return the number of occurrences (term frequency) for every term\n",
    "    def tf_per_year(self, period=None):\n",
    "        rows, cols = self.matrix.nonzero()\n",
    "        res = {m:Counter() for m in rows}\n",
    "        for m, n in zip(rows, cols):\n",
    "            year = self.year[n]\n",
    "            if period == None or period[0] <= year <= period[1]:\n",
    "                res[m][year] += self.matrix[m, n]\n",
    "            \n",
    "        return res\n",
    "    \n",
    "    def plot_tf_series(self, term, period, relative=False):\n",
    "        m = self.term_codes.get_code(term)\n",
    "        if relative:\n",
    "            norm = Counter(self.year)\n",
    "        else:\n",
    "            norm = Counter(set(self.year))\n",
    "            \n",
    "        if m:\n",
    "            row = self.matrix.getrow(m)\n",
    "            _, cols = row.nonzero()\n",
    "            c = Counter()\n",
    "            for n in cols:\n",
    "                year = self.year[n]\n",
    "                if period == None or period[0] <= year <= period[1]:\n",
    "                    c[year] += row[0, n]\n",
    "            \n",
    "            X = sorted(c.keys())\n",
    "            Y = [c[x] / norm[x] for x in X]\n",
    "            plt.plot(X, Y, 'o-')\n",
    "            plt.ylim(0, 1.2 * max(Y))\n",
    "            plt.title(term)       \n",
    "        else:\n",
    "            raise ValueError('{} is not in store'.format(term))\n",
    "             \n",
    "    # return dictionary with a list of text-years per term \n",
    "    # period = pair of years (min _year, max_year) inclusive\n",
    "    # keep_all = true if unlisted texts are not ignored\n",
    "    def document_years(self, period=None, keep_all=True):\n",
    "        rows, cols = self.matrix.nonzero()\n",
    "        res = {m:list() for m in rows}\n",
    "        for m, n in zip(rows, cols):\n",
    "            if keep_all or self.listed[n]:\n",
    "                year = self.year[n]\n",
    "                print(year)\n",
    "                if period == None or period[0] <= year <= period[1]:\n",
    "                    res[m].append(year)\n",
    "        \n",
    "        return res\n",
    "    \n",
    "    # return dictionary with Counter of abstract-years per term\n",
    "    def df_per_year(self, period=None, keep_all=True):\n",
    "        doc_years = self.document_years(period, keep_all)\n",
    "        \n",
    "        return {m:Counter(v) for m, v in doc_years.items()}\n",
    "    \n",
    "    # create a plot with document frequency of terms\n",
    "    def plot_df(self, terms, period, keep_all=True):\n",
    "        dfs = self.df_per_year(period, keep_all)\n",
    "        for term in terms:\n",
    "            m = self.term_codes.get_code(term.lower())\n",
    "            df = dfs[m] \n",
    "            X = range(*period)\n",
    "            Y = [df.get(x, 0) for x in X]\n",
    "            plt.clf()\n",
    "            plt.plot(X, Y)\n",
    "            plt.title(term)\n",
    "            filename = 'plots/{}.png'.format(term)\n",
    "            print('Saving', filename)\n",
    "            plt.savefig(filename, dpi=200)\n",
    "            \n",
    "    # compute the average age in the specified period of documents containing \n",
    "    # each term with global term-frequency above tf_threshold\n",
    "    # and annual document frequency above df_threshold (one year at least)\n",
    "    # period = optional pair of years (min _year, max_year) inclusive\n",
    "    def get_ages(self, period=None, \n",
    "                 tf_threshold=20, df_threshold=3, keep_all=True):\n",
    "        res = dict()\n",
    "        doc_years = self.document_years(period, keep_all)\n",
    "        for m, values in doc_years.items():\n",
    "            term = self.term_codes.get_term(m)\n",
    "            if len(values) > 0:\n",
    "                df = Counter(values).most_common(1)[0][1]\n",
    "                tf = self.term_frequencies[m]\n",
    "                #break;\n",
    "                if df >= df_threshold and tf >= tf_threshold:       \n",
    "                    res[term] = mean(values)\n",
    "        return res\n",
    "    \n",
    "    # return abstract numbers containing any term in this set of terms\n",
    "    def docs_with_term(self, terms, period=None):\n",
    "        rows, cols = self.matrix.nonzero()\n",
    "        res = set()\n",
    "        for m, n in zip(rows, cols):\n",
    "            term  = self.term_codes.get_term(m)\n",
    "            if terms == None or term in terms:\n",
    "                year = self.year[n]\n",
    "                if period == None or period[0] <= year <= period[1]:\n",
    "                     res.add(n)\n",
    "                \n",
    "        return res\n",
    "            \n",
    "       \n",
    "    def search(self, term):\n",
    "        m = self.term_codes.get_code(term)\n",
    "        docs = self.matrix.getrow(m).nonzero()[1]\n",
    "        \n",
    "        return [(self.year[n], self.type[n], self.panel[n]) for n in docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = articles_2010_2021"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[data.TITLE.str.len() > 40]   \n",
    "\n",
    "print('Processing', len(data), 'texts')\n",
    "\n",
    "s = Sample(data, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('sample-dblp.pkl', 'wb') as f:\n",
    "    pickle.dump(s, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('sample-dblp.pkl', 'rb') as f:\n",
    "    s = pickle.load(f)\n",
    "print('Loaded stats for', len(s), 'texts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "period = (2015, 2020)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ages = s.get_ages(period)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top = pd.DataFrame.from_dict(ages, orient='index').reset_index()\n",
    "print(top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top.columns = ['TERM', 'AGE']\n",
    "#top = top.sort_values('AGE', ascending=False).head(250)   \n",
    "top = top.sort_values('AGE', ascending=False).head(50)   \n",
    "top['DOC FREQ'] = top.TERM.apply(s.get_df)\n",
    "top['TERM FREQ'] = top.TERM.apply(s.get_tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare to export\n",
    "top['TERM'] = top.TERM.apply(s.most_frequent_capitalization)\n",
    "print(top.set_index('TERM').head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts = pd.datetime.now().strftime(\"%Y-%m-%d_%H.%M\")    \n",
    "filename = 'output/vocabulary_{}.xlsx'.format(ts)\n",
    "with pd.ExcelWriter(filename) as writer:\n",
    "    top.set_index('TERM').to_excel(writer, sheet_name='terms')\n",
    "\n",
    "print('vocabulary saved to', filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "https://doi.org/10.23636/1344"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
